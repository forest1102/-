{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/forest1102/-/blob/master/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es2eG3GDp7bG",
        "colab_type": "text"
      },
      "source": [
        "Requirements:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0cMAhVHn1WN",
        "colab_type": "code",
        "outputId": "06070f4d-1b5b-4ad5-e8be-bbb3214a098e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0b1 cupy numpy"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0b1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/53/e18c5e7a2263d3581a979645a185804782e59b8e13f42b9c3c3cfb5bb503/tensorflow_gpu-2.0.0b1-cp36-cp36m-manylinux1_x86_64.whl (348.9MB)\n",
            "\u001b[K     |████████████████████████████████| 348.9MB 42kB/s \n",
            "\u001b[?25hCollecting cupy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/89/99f980706c61e6b96a579a81dea3eb68c22df1b526bf357673be5e18fe31/cupy-7.0.0.tar.gz (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 30.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.17.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0b1) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0b1) (3.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0b1) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0b1) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0b1) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0b1) (1.1.0)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/dd/99c47dd007dcf10d63fd895611b063732646f23059c618a373e85019eb0e/tf_estimator_nightly-1.14.0.dev2019060501-py2.py3-none-any.whl (496kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 37.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0b1) (1.1.0)\n",
            "Collecting tb-nightly<1.14.0a20190604,>=1.14.0a20190603\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/96/571b875cd81dda9d5dfa1422a4f9d749e67c0a8d4f4f0b33a4e5f5f35e27/tb_nightly-1.14.0a20190603-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 31.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0b1) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0b1) (0.1.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0b1) (1.11.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0b1) (0.33.6)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0b1) (0.2.2)\n",
            "Requirement already satisfied: fastrlock>=0.3 in /usr/local/lib/python3.6/dist-packages (from cupy) (0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0b1) (42.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0b1) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0b1) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0b1) (3.1.1)\n",
            "Building wheels for collected packages: cupy\n",
            "  Building wheel for cupy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cupy: filename=cupy-7.0.0-cp36-cp36m-linux_x86_64.whl size=28607969 sha256=f5a5709f422ad45d7d56c7777fff228c51dc4f910bd4633b4690d43faf49fbac\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/33/37/f412224f7550a11ee18a9f41f7aff28df380bfa994f7280ab3\n",
            "Successfully built cupy\n",
            "Installing collected packages: tf-estimator-nightly, tb-nightly, tensorflow-gpu, cupy\n",
            "Successfully installed cupy-7.0.0 tb-nightly-1.14.0a20190603 tensorflow-gpu-2.0.0b1 tf-estimator-nightly-1.14.0.dev2019060501\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br1fUhRtAQA8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c11cdff2-4692-440d-9521-8150a4656512"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "print(tf.reduce_sum(tf.random.normal([1000, 1000])))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(-67.05016, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S96c8033qwSx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir dataset\n",
        "!wget https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt -P /content/dataset\n",
        "!wget https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.test.txt -P /content/dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMyoc8bv5D1A",
        "colab_type": "code",
        "outputId": "cfb083f7-2cad-4e6d-ec43-50760f9b0473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Embedding, Lambda, Input, Reshape, Dot, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "import pickle\n",
        "\n",
        "from util import create_contexts_target, most_similar\n",
        "from dataset import ptb\n",
        "from negative_sampling import generate_with_negative_sample\n",
        "\n",
        "tensorboard_callback = TensorBoard(log_dir='logs/cbow')\n",
        "\n",
        "window_size = 10\n",
        "hidden_size = 100\n",
        "batch_size = 100\n",
        "max_epoch = 15\n",
        "sample_size = 5\n",
        "\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "test_corpus = ptb.load_data('test')[0]\n",
        "\n",
        "vocab_size = len(word_to_id)\n",
        "contexts, target = create_contexts_target(corpus, window_size)\n",
        "test_contexts, test_target = create_contexts_target(test_corpus, window_size)\n",
        "\n",
        "print('corpus', corpus.shape)\n",
        "print('contexts', contexts.shape)\n",
        "print('target', target.shape)\n",
        "\n",
        "\n",
        "contexts_input = Input(shape=(window_size * 2,), name='contexts_input')\n",
        "target_input = Input(shape=(1,), name='target_input')\n",
        "\n",
        "embed = Embedding(vocab_size, hidden_size, input_length=window_size * 2)\n",
        "\n",
        "contexts_embed = embed(contexts_input)\n",
        "contexts_hidden = Lambda(lambda arr: K.mean(arr, axis=1))(contexts_embed)\n",
        "\n",
        "target_embed = Embedding(vocab_size, hidden_size, input_length=1)(target_input)\n",
        "target_hidden = Reshape((hidden_size, ))(target_embed)\n",
        "\n",
        "embed_dot = Dot(axes=1)([contexts_hidden, target_hidden])\n",
        "output = Dense(1, activation='sigmoid')(embed_dot)\n",
        "\n",
        "model = Model(inputs=[contexts_input, target_input], outputs=output)\n",
        "print('corpus', corpus.shape)\n",
        "print('contexts', contexts.shape)\n",
        "print('target', target.shape)\n",
        "\n",
        "model.compile(\n",
        "    optimizer='Adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['acc']\n",
        ")\n",
        "\n",
        "print(model.summary())\n",
        "hist = model.fit_generator(\n",
        "    generate_with_negative_sample(\n",
        "        corpus, contexts, target, batch_size, sample_size=sample_size),\n",
        "    steps_per_epoch=len(contexts) // batch_size,\n",
        "    initial_epoch=0,\n",
        "    epochs=max_epoch, callbacks=[tensorboard_callback],\n",
        ")\n",
        "\n",
        "word_vecs = model.get_weights()[0]\n",
        "params = {}\n",
        "params['word_vecs'] = word_vecs.astype(np.float16)\n",
        "params['word_to_id'] = word_to_id\n",
        "params['id_to_word'] = id_to_word\n",
        "pkl_file = 'cbow_params.pkl'  # or 'skipgram_params.pkl'\n",
        "with open(pkl_file, 'wb') as f:\n",
        "    pickle.dump(params, f, -1)\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d38b1c8248c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_contexts_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mptb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnegative_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenerate_with_negative_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'util'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}